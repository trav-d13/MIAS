{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to MIAS Documentation","text":"<p>MIAS is the Musically Illiterate Aid System, designed to  aid artists add to their performance playlist by finding similar songs. </p> <p>The following links provide access to the hosted MIAS service and source code: </p> <ul> <li>MIAS: https://mias-recommender-sys.streamlit.app/</li> <li>Source Code: https://github.com/trav-d13/MIAS</li> </ul> <p></p> <p>This project encourages collaboration! </p> <p>Whether you are new to recommender systems or a pro. The project embraced a modular design so that you can test out your own data pipelines and similarity metrics in a way where you can easily see and deploy your results</p>"},{"location":"#mias-documentation","title":"MIAS Documentation","text":"<p>This section provides access to the extensive documentation underlying the MIAS project.</p>"},{"location":"#recommender-system-processes","title":"Recommender System Processes","text":""},{"location":"#data-processing","title":"Data Processing","text":""},{"location":"#similarity-interface","title":"Similarity Interface","text":""},{"location":"#cosine-similarity","title":"Cosine Similarity","text":""},{"location":"#pipeline-interface","title":"Pipeline interface","text":""},{"location":"#cosine-pipeline","title":"Cosine Pipeline","text":""},{"location":"#application-ui","title":"Application UI","text":""},{"location":"#recommender","title":"Recommender","text":""},{"location":"#dataset","title":"Dataset","text":""},{"location":"data_processing/","title":"Data processing","text":""},{"location":"data_processing/#data-processing","title":"Data Processing","text":"<p>This file forms the basis of Spotify data processing.</p>"},{"location":"data_processing/#data-processing-documentation","title":"Data Processing Documentation","text":""},{"location":"data_processing/#src.data_processing.add_playlist_tracking","title":"<code>add_playlist_tracking(name, store)</code>","text":"<p>Method assigns the playlist name to each of the tracks to allow for traceback</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the playlist</p> required <code>store</code> <code>dict</code> <p>The object in which to store extracted information</p> required Source code in <code>src/data_processing.py</code> <pre><code>def add_playlist_tracking(name, store):\n    \"\"\"Method assigns the playlist name to each of the tracks to allow for traceback\n\n    Args:\n        name (str): The name of the playlist\n        store (dict): The object in which to store extracted information\n    \"\"\"\n    store['playlist_name'] = [name] * len(store['uris'])\n</code></pre>"},{"location":"data_processing/#src.data_processing.construct_storage","title":"<code>construct_storage()</code>","text":"<p>Method constructs the storage dictionary in which collected track information is stored during collection, and enables saving as a csv file.</p> <p>Returns:</p> Type Description <code>dict</code> <p>An dictionary with each of the features initialized as keys, with associated empty list values.</p> Source code in <code>src/data_processing.py</code> <pre><code>def construct_storage():\n    \"\"\"Method constructs the storage dictionary in which collected track information is stored during collection,\n    and enables saving as a csv file.\n\n    Returns:\n        (dict): An dictionary with each of the features initialized as keys, with associated empty list values.\n    \"\"\"\n    store_outline = {\n        'uris': [],\n        'names': [],\n        'artist_names': [],\n        'artist_uris': [],\n        'artist_pop': [],\n        'artist_genres': [],\n        'albums': [],\n        'track_pop': [],\n        'danceability': [],\n        'energy': [],\n        'keys': [],\n        'loudness': [],\n        'modes': [],\n        'speechiness': [],\n        'acousticness': [],\n        'instrumentalness': [],\n        'liveness': [],\n        'valences': [],\n        'tempos': [],\n        'types': [],\n        'ids': [],\n        'track_hrefs': [],\n        'analysis_urls': [],\n        'durations_ms': [],\n        'time_signatures': [],\n        'playlist_name': []\n    }\n    return store_outline\n</code></pre>"},{"location":"data_processing/#src.data_processing.extract_artist_info","title":"<code>extract_artist_info(store, sp)</code>","text":"<p>Method deals with extracting artist information from the <code>artists()</code> API call through Spotipy</p> <p>Parameters:</p> Name Type Description Default <code>store</code> <code>dict</code> <p>The object in which to store extracted information</p> required <code>sp</code> <code>Spotipy Authorization</code> <p>The authorized spotipy credentials object</p> required Source code in <code>src/data_processing.py</code> <pre><code>def extract_artist_info(store, sp):\n    \"\"\"Method deals with extracting artist information from the `artists()` API call through Spotipy\n\n    Args:\n        store (dict): The object in which to store extracted information\n        sp (Spotipy Authorization): The authorized spotipy credentials object\n    \"\"\"\n    limit = 50\n    offset = 0\n    while offset &lt; len(store['artist_uris']):\n        if offset + limit &gt; len(store['artist_uris']):  # If else, deals with batching\n            artists_info = sp.artists(store['artist_uris'][offset: len(store['artist_uris'])])  # Gather artis info through API\n        else:\n            artists_info = sp.artists(store['artist_uris'][offset: offset + limit])  # Gather artis info through API\n\n        for artist in artists_info['artists']:  # Extract popularity and genres from each artist\n            store['artist_pop'].append(artist['popularity'])  # Access artist popularity\n            store['artist_genres'].append(artist['genres'])  # Access artist genres\n\n        offset = offset + limit\n</code></pre>"},{"location":"data_processing/#src.data_processing.extract_audio_features","title":"<code>extract_audio_features(store, sp)</code>","text":"<p>Method deal with extracting audio analysis features for a given batch of tracks</p> <p>Parameters:</p> Name Type Description Default <code>store</code> <code>dict</code> <p>The object in which to store extracted information</p> required <code>sp</code> <code>Spotipy Authorization</code> <p>The authorized spotipy credentials object</p> required Source code in <code>src/data_processing.py</code> <pre><code>def extract_audio_features(store, sp):\n    \"\"\"Method deal with extracting audio analysis features for a given batch of tracks\n\n    Args:\n        store (dict): The object in which to store extracted information\n        sp (Spotipy Authorization): The authorized spotipy credentials object\n    \"\"\"\n    limit = 100\n    offset = 0\n    while offset &lt; len(store['uris']):\n        if offset + limit &gt; len(store['uris']):   # If else, deals with batching of acoustic features\n            track_info = sp.audio_features(store['uris'][offset: len(store['uris'])])\n        else:\n            track_info = sp.audio_features(store['uris'][offset: offset + limit])\n\n        for track in track_info:  # For each track extract the necessary features and store it\n            store['danceability'].append(track['danceability'])\n            store['energy'].append(track['energy'])\n            store['keys'].append(track['key'])\n            store['loudness'].append(track['loudness'])\n            store['modes'].append(track['mode'])\n            store['speechiness'].append(track['speechiness'])\n            store['acousticness'].append(track['acousticness'])\n            store['instrumentalness'].append(track['instrumentalness'])\n            store['liveness'].append(track['liveness'])\n            store['valences'].append(track['valence'])\n            store['tempos'].append(track['tempo'])\n            store['types'].append(track['type'])\n            store['ids'].append(track['id'])\n            store['track_hrefs'].append(track['track_href'])\n            store['analysis_urls'].append(track['analysis_url'])\n            store['durations_ms'].append(track['duration_ms'])\n            store['time_signatures'].append(track['time_signature'])\n\n        offset = offset + limit\n</code></pre>"},{"location":"data_processing/#src.data_processing.extract_tracks","title":"<code>extract_tracks(sp, playlist_uri, store)</code>","text":"<p>Method deals with extracting tracks from a given playlist Note, this method forms the cornerstone of extraction, providing track access from a playlist.</p> <p>Parameters:</p> Name Type Description Default <code>sp</code> <code>Spotipy Authorization</code> <p>The authorized spotipy credentials object</p> required <code>playlist_uri</code> <code>str</code> <p>The URI of the Spotify playlist</p> required <code>store</code> <code>dict</code> <p>The object in which to store extracted information</p> required Source code in <code>src/data_processing.py</code> <pre><code>def extract_tracks(sp, playlist_uri, store):\n    \"\"\"Method deals with extracting tracks from a given playlist\n    Note, this method forms the cornerstone of extraction, providing track access from a playlist.\n\n    Args:\n        sp (Spotipy Authorization): The authorized spotipy credentials object\n        playlist_uri (str): The URI of the Spotify playlist\n        store (dict): The object in which to store extracted information\n    \"\"\"\n    offset = 0\n    limit = 100\n    playlist = sp.playlist_tracks(playlist_uri, limit=2, offset=offset)  # Retrieve the initial batch of songs\n    total_songs = playlist['total']  # Extract the total number of songs\n\n    while offset &lt; total_songs:\n        time.sleep(2)\n        playlist = sp.playlist_tracks(playlist_uri, limit=100, offset=offset)  # Retrieve batch of songs in playlist\n        store = retrieve_batch_info(playlist, store)  # Retrieve batch information\n        offset = offset + limit  # Update offset\n\n    extract_artist_info(store, sp)  # Extract the artist features for each track\n    extract_audio_features(store, sp)  # Extract the audio features for each track\n</code></pre>"},{"location":"data_processing/#src.data_processing.find_top_playlists","title":"<code>find_top_playlists(sp, country)</code>","text":"<p>Method finds the top-20 playlists in a given country</p> <p>Parameters:</p> Name Type Description Default <code>sp</code> <code>Spotipy Authorization</code> <p>The authorized spotipy credentials object</p> required <code>country</code> <code>str</code> <p>The ISO 3166-1 alpha-2 country code of where the playlist should be extracted from.</p> required <p>Returns:</p> Name Type Description <code>uris</code> <code>list</code> <p>A list of uris linking to each of the found playlists</p> <code>names</code> <code>list</code> <p>A related list of playlist names corresponding to the uris</p> Source code in <code>src/data_processing.py</code> <pre><code>def find_top_playlists(sp, country):\n    \"\"\"Method finds the top-20 playlists in a given country\n\n    Args:\n        sp (Spotipy Authorization): The authorized spotipy credentials object\n        country (str): The ISO 3166-1 alpha-2 country code of where the playlist should be extracted from.\n\n    Returns:\n        uris (list): A list of uris linking to each of the found playlists\n        names (list): A related list of playlist names corresponding to the uris\n    \"\"\"\n    uris = []\n    names = []\n    playlists = sp.featured_playlists(country=country, limit=20)\n    playlist_items = playlists['playlists']['items']\n\n    for item in playlist_items:  # Extract the uri and name from each playlist\n        uris.append(item['uri'].split(':')[-1])\n        names.append(item['name'])\n    return uris, names\n</code></pre>"},{"location":"data_processing/#src.data_processing.merge_stores","title":"<code>merge_stores(tracks_store, store)</code>","text":"<p>Method deals with merging one store into another Args:     store (dict): The track storage object to be merged.     tracks_store (dict): The larger object in which to merge store into</p> Source code in <code>src/data_processing.py</code> <pre><code>def merge_stores(tracks_store, store):\n    \"\"\"Method deals with merging one store into another\n    Args:\n        store (dict): The track storage object to be merged.\n        tracks_store (dict): The larger object in which to merge store into\n    \"\"\"\n    for key, value in store.items():\n        tracks_store[key].extend(value)\n</code></pre>"},{"location":"data_processing/#src.data_processing.process_items","title":"<code>process_items(store, items)</code>","text":"<p>This method extracts the information from the Spotipy <code>tracks()</code> API call. Information includes: - Track uri - Track name - Track album - Track popularity</p> <p>Parameters:</p> Name Type Description Default <code>store</code> <code>dict</code> <p>The object in which to store extracted information</p> required <code>items</code> <code>list</code> <p>The list of track information generated from the Spotipy tracks API call</p> required <p>Returns:</p> Type Description <code>dict</code> <p>An updated store of information</p> Source code in <code>src/data_processing.py</code> <pre><code>def process_items(store, items):\n    \"\"\"This method extracts the information from the Spotipy `tracks()` API call.\n    Information includes:\n    - Track uri\n    - Track name\n    - Track album\n    - Track popularity\n\n    Args:\n        store (dict): The object in which to store extracted information\n        items (list): The list of track information generated from the Spotipy tracks API call\n\n    Returns:\n        (dict): An updated store of information\n\n    \"\"\"\n    for item in items:\n        track_uri = item['track']['uri'].split(':')[-1]  # Extract track uri only from the provided link\n        store['uris'].append(track_uri)  # Retrieve track uri\n        store['names'].append(item['track']['name'])  # Retrieve track name\n\n        store['artist_uris'].append(item['track']['artists'][0]['uri'].split(':')[-1])  # Find artist uri\n\n        store['artist_names'].append(item['track']['artists'][0]['name'])  # Access artist name\n\n        store['albums'].append(item['track']['album']['name'])  # Access album names\n        store['track_pop'].append(item['track']['popularity'])  # Access track popularity\n    return store\n</code></pre>"},{"location":"data_processing/#src.data_processing.retrieve_batch_info","title":"<code>retrieve_batch_info(playlist, store)</code>","text":"<p>Method retrieves the essential information from the batch API call to enable simplified extraction</p> <p>Returns:</p> Type Description <code>dict</code> <p>A partially updated store of track information. This required further data extraction.</p> Source code in <code>src/data_processing.py</code> <pre><code>def retrieve_batch_info(playlist, store):\n    \"\"\"Method retrieves the essential information from the batch API call to enable simplified extraction\n\n    Returns:\n        (dict): A partially updated store of track information. This required further data extraction.\n    \"\"\"\n    items = playlist['items']  # Extract items (A list containing information on the tracks)\n    store = process_items(store, items)  # Extract info and store it.\n    return store\n</code></pre>"},{"location":"data_processing/#src.data_processing.save_data","title":"<code>save_data(tracks_store, name='tracks.csv')</code>","text":"<p>Method deals with saving collected track data</p> <p>Note, this method removes all duplicate tracks, such that all tracks within the dataset are unique, always keeping most up-to-date representation of each track.</p> <p>Parameters:</p> Name Type Description Default <code>tracks_store</code> <code>dict</code> <p>The dictionary containing all information extracted about the tracks</p> required <code>name</code> <code>str</code> <p>The name of the file to save the information to. Default is the tracks.csv dataset file.</p> <code>'tracks.csv'</code> Source code in <code>src/data_processing.py</code> <pre><code>def save_data(tracks_store, name='tracks.csv'):\n    \"\"\"Method deals with saving collected track data\n\n    Note, this method removes all duplicate tracks, such that all tracks within the dataset are unique, always keeping\n    most up-to-date representation of each track.\n\n    Args:\n        tracks_store (dict): The dictionary containing all information extracted about the tracks\n        name (str): The name of the file to save the information to. Default is the tracks.csv dataset file.\n\n    \"\"\"\n    root_path = os.path.abspath(os.path.dirname(os.path.dirname(__file__)))\n    file_path = os.path.join(root_path, 'data', name)\n    df_new = pd.DataFrame.from_dict(tracks_store)  # Create a dataframe from the collected data\n    df_old = pd.read_csv(file_path, index_col=0)  # Create dataframe from old values\n\n    if df_old.shape[0] != 0 and name == \"tracks.csv\":  # Previously saved songs, requiring further processing to have unique values only\n        df_combined = pd.concat([df_new, df_old], axis=0)\n        df_unique = df_combined.drop_duplicates(subset='uris', keep='first')  # Drop duplicates (keeping most up to date)\n        df_unique = df_unique.reset_index(drop=True)\n        df_unique.to_csv(file_path, mode='w')\n    else:\n        df_new.to_csv(file_path, mode='w')\n</code></pre>"},{"location":"data_processing/#src.data_processing.target_playlist_extraction","title":"<code>target_playlist_extraction(sp, url, name)</code>","text":"<p>This method extracts all track information from a given target playlist Args:     sp (Spotipy Authorization): The authorized spotipy credentials object     url (str): The url of the playlist from which to extract information     name (str): The name of the playlist</p> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary containing all features and information pertaining to the target playlist.</p> Source code in <code>src/data_processing.py</code> <pre><code>def target_playlist_extraction(sp, url, name):\n    \"\"\"This method extracts all track information from a given target playlist\n    Args:\n        sp (Spotipy Authorization): The authorized spotipy credentials object\n        url (str): The url of the playlist from which to extract information\n        name (str): The name of the playlist\n\n    Returns:\n        (dict): A dictionary containing all features and information pertaining to the target playlist.\n    \"\"\"\n    uri = url2uri(url)  # Extract the uri\n    store = construct_storage()  # Create the info storage\n    extract_tracks(sp, uri, store)  # Extract track information\n    add_playlist_tracking(name, store)  # Add playlist information (name)\n    save_data(store, 'target.csv')  # Save the data (Update tracks.csv) dataset\n    return store\n</code></pre>"},{"location":"data_processing/#src.data_processing.top_playlist_extraction","title":"<code>top_playlist_extraction(sp)</code>","text":"<p>Method extracts the tracks in the 20 top-performing playlists from a selection of countries The countries include: Australia, UK, USA, Canada, Jamaica, South Africa</p> <p>This method does not return any information, but stores it in the tracks.csv dataset file.</p> <p>Parameters:</p> Name Type Description Default <code>sp</code> <code>Spotipy Authorization</code> <p>The authorized spotipy credentials object</p> required Source code in <code>src/data_processing.py</code> <pre><code>def top_playlist_extraction(sp):\n    \"\"\"Method extracts the tracks in the 20 top-performing playlists from a selection of countries\n     The countries include: Australia, UK, USA, Canada, Jamaica, South Africa\n\n     This method does not return any information, but stores it in the tracks.csv dataset file.\n\n     Args:\n         sp (Spotipy Authorization): The authorized spotipy credentials object\n    \"\"\"\n    countries = ['AU', 'GB', 'US', 'CA', 'JM', 'ZA']\n\n    tracks_store = construct_storage()  # Construct track info storage\n\n    for country in countries:  # Iterate through countries\n        print(f'Country: {country}')\n        top_playlists, names = find_top_playlists(sp, country)  # Find top 20 playlists in each country\n\n        for playlist, name in zip(top_playlists[:], names[:]):  # Iterate through playlists\n            try:\n                print(f'Playlist name: {name}')\n                store = construct_storage()\n                extract_tracks(sp, playlist, store)\n                add_playlist_tracking(name, store)\n                merge_stores(tracks_store, store)  # Merge the playlist information, by merging the data stored.\n                time.sleep(2)  # Respect APi limits through a forced sleep\n            except Exception:\n                print(f\"Error accessing playlist {name} tracks\")\n        print('-----------------------------------------------------------------------------')\n\n    save_data(tracks_store)  # Save the data\n</code></pre>"},{"location":"data_processing/#src.data_processing.update_tracking","title":"<code>update_tracking(df)</code>","text":"<p>Method updates the <code>dataset_growth.csv</code> file when new tracks are added to the dataset to record dataset growth</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The dataframe containing all stored tracks, including new additions</p> required Source code in <code>src/data_processing.py</code> <pre><code>def update_tracking(df):\n    \"\"\"Method updates the `dataset_growth.csv` file when new tracks are added to the dataset to record dataset growth\n\n    Args:\n        df (DataFrame): The dataframe containing all stored tracks, including new additions\n    \"\"\"\n    root_path = os.path.abspath(os.path.dirname(os.path.dirname(__file__)))  # Read in the growth dataset\n    file_path = os.path.join(root_path, 'data', 'dataset_growth.csv')\n    tracking_df = pd.read_csv(file_path, index_col=0)\n\n    current_time = datetime.now()  # Extract new features to update the growth dataset with\n    new_length = df.shape[0]\n    new_entry = pd.DataFrame.from_dict({'date': [current_time.strftime(\"%d-%m-%Y\")],\n                                        'time': [current_time.strftime(\"%H:%M:%S\")],\n                                        'track_count': [new_length]})\n\n    tracking_df = pd.concat([tracking_df, new_entry], axis=0, ignore_index=True)  # Update growth dataset\n    tracking_df.reset_index(drop=True)\n    tracking_df.to_csv(file_path, mode='w')\n</code></pre>"},{"location":"data_processing/#src.data_processing.url2uri","title":"<code>url2uri(url)</code>","text":"<p>Method extracts the uri from a given Spotify url</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The Spotify playlist url</p> required <p>Returns:     (str): The uri of the Spotify playlist</p> Source code in <code>src/data_processing.py</code> <pre><code>def url2uri(url):\n    \"\"\"Method extracts the uri from a given Spotify url\n\n    Args:\n        url (str): The Spotify playlist url\n    Returns:\n        (str): The uri of the Spotify playlist\n    \"\"\"\n    return url.split('/')[-1].split('?')[0]\n</code></pre>"},{"location":"dataset/","title":"Dataset","text":""},{"location":"dataset/#dataset-page","title":"Dataset Page","text":"<p>The dataset page (Visible using left side bar) provides a dataset analytics UI and data download platform.</p>"},{"location":"dataset/#dataset-page-documentation","title":"Dataset Page Documentation","text":""},{"location":"dataset/#src.pages.dataset.Monitor","title":"<code>Monitor</code>","text":"<p>Monitor class serves as a dataset monitor</p> <p>Attributes:</p> Name Type Description <code>history_name</code> <code>str</code> <p>Name of the history dataset file name</p> <code>tracks_name</code> <code>str</code> <p>Name of the tracks dataset file name</p> <code>root_path</code> <code>Path</code> <p>Path to the root of the project</p> <code>hist_path</code> <code>Path</code> <p>Path to the history dataset</p> <code>track_path</code> <code>Path</code> <p>Path to the tracks dataset</p> <code>history</code> <code>DataFrame</code> <p>History dataset as a dataframe</p> <code>tracks</code> <code>DataFrame</code> <p>Track dataset as a dataframe</p> Source code in <code>src/pages/dataset.py</code> <pre><code>class Monitor:\n    \"\"\"Monitor class serves as a dataset monitor\n\n    Attributes:\n        history_name (str): Name of the history dataset file name\n        tracks_name (str): Name of the tracks dataset file name\n        root_path (Path): Path to the root of the project\n        hist_path (Path): Path to the history dataset\n        track_path (Path): Path to the tracks dataset\n        history (DataFrame): History dataset as a dataframe\n        tracks (DataFrame): Track dataset as a dataframe\n    \"\"\"\n    def __init__(self):\n        \"\"\"Method constructs the dataset monitor for data analysis within this page\"\"\"\n        self.history_name = 'dataset_growth.csv'\n        self.tracks_name = 'tracks.csv'\n        self.root_path = os.path.abspath(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))\n        self.hist_path = os.path.join(self.root_path, 'data', self.history_name)\n        self.track_path = os.path.join(self.root_path, 'data', self.tracks_name)\n\n        self.history = pd.read_csv(self.hist_path)\n        self.tracks = pd.read_csv(self.track_path)\n\n        self.history['date'] = pd.to_datetime(self.history['date'], format=\"%d-%m-%Y\")  # Format the date\n        self.history['time'] = pd.to_datetime(self.history['time'], format='%H:%M:%S')  # Format the time\n\n    def determine_date_range(self):\n        \"\"\"Method determines the date range in the dataset history file.\n\n        Returns:\n            start (datetime): The earliest date available in the dataset history file.\n            end (datetime): The latest date available in the dataset history file.\n        \"\"\"\n        return self.history['date'].min(), self.history['date'].max()\n\n    def access_acoustic_sample_features(self):\n        \"\"\"Method samples the tracks dataset and selects acoustic features for visualization in the pairplot.\n\n        Note, sampling is used here as the size of the tracks dataset would take a long time to render in a pairplot.\n\n        Returns:\n            (DataFrame): Sampled tracks dataframe containing select acoustic features.\n        \"\"\"\n        track_sample = self.tracks.sample(frac=0.1, random_state=1)\n        acoustics_df = track_sample[['danceability', 'energy', 'loudness', 'speechiness',\n                                    'acousticness', 'instrumentalness']]\n        return acoustics_df\n\n    def access_specific_features(self, selection: list, sample=True):\n        \"\"\"Method allows for access to specific acoustic features in the tracks dataset\n\n        Args:\n            selection (list): A list of acoustic features\n            sample (bool): If True, the features must be sampled. If False, the feature is not sampled.\n\n        Returns:\n            (DataFrame): A dataframe containing the specified feature.\n        \"\"\"\n        if sample:\n            track_sample = self.tracks.sample(frac=0.1, random_state=2)\n        else:\n            track_sample = self.tracks\n        return track_sample[selection]\n\n    def access_artist_names(self):\n        \"\"\"Method determines the unique artist names in the tracks dataset.\n\n        Returns:\n            (list): A list of unique artists available in the tracks dataset.\n        \"\"\"\n        names = self.tracks['artist_names'].unique().tolist()\n        return names\n\n    def access_feature_definitions(self):\n        \"\"\"Method allows for the `data/feature_def` file to be read-in providing feature definitions\n\n        Returns:\n            (str): Feature definitions\n        \"\"\"\n        def_file = os.path.join(self.root_path, 'data', 'feature_def.txt')\n        with open(def_file, 'r') as file:\n            contents = file.read()\n            return contents\n</code></pre>"},{"location":"dataset/#src.pages.dataset.Monitor.__init__","title":"<code>__init__()</code>","text":"<p>Method constructs the dataset monitor for data analysis within this page</p> Source code in <code>src/pages/dataset.py</code> <pre><code>def __init__(self):\n    \"\"\"Method constructs the dataset monitor for data analysis within this page\"\"\"\n    self.history_name = 'dataset_growth.csv'\n    self.tracks_name = 'tracks.csv'\n    self.root_path = os.path.abspath(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))\n    self.hist_path = os.path.join(self.root_path, 'data', self.history_name)\n    self.track_path = os.path.join(self.root_path, 'data', self.tracks_name)\n\n    self.history = pd.read_csv(self.hist_path)\n    self.tracks = pd.read_csv(self.track_path)\n\n    self.history['date'] = pd.to_datetime(self.history['date'], format=\"%d-%m-%Y\")  # Format the date\n    self.history['time'] = pd.to_datetime(self.history['time'], format='%H:%M:%S')  # Format the time\n</code></pre>"},{"location":"dataset/#src.pages.dataset.Monitor.access_acoustic_sample_features","title":"<code>access_acoustic_sample_features()</code>","text":"<p>Method samples the tracks dataset and selects acoustic features for visualization in the pairplot.</p> <p>Note, sampling is used here as the size of the tracks dataset would take a long time to render in a pairplot.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Sampled tracks dataframe containing select acoustic features.</p> Source code in <code>src/pages/dataset.py</code> <pre><code>def access_acoustic_sample_features(self):\n    \"\"\"Method samples the tracks dataset and selects acoustic features for visualization in the pairplot.\n\n    Note, sampling is used here as the size of the tracks dataset would take a long time to render in a pairplot.\n\n    Returns:\n        (DataFrame): Sampled tracks dataframe containing select acoustic features.\n    \"\"\"\n    track_sample = self.tracks.sample(frac=0.1, random_state=1)\n    acoustics_df = track_sample[['danceability', 'energy', 'loudness', 'speechiness',\n                                'acousticness', 'instrumentalness']]\n    return acoustics_df\n</code></pre>"},{"location":"dataset/#src.pages.dataset.Monitor.access_artist_names","title":"<code>access_artist_names()</code>","text":"<p>Method determines the unique artist names in the tracks dataset.</p> <p>Returns:</p> Type Description <code>list</code> <p>A list of unique artists available in the tracks dataset.</p> Source code in <code>src/pages/dataset.py</code> <pre><code>def access_artist_names(self):\n    \"\"\"Method determines the unique artist names in the tracks dataset.\n\n    Returns:\n        (list): A list of unique artists available in the tracks dataset.\n    \"\"\"\n    names = self.tracks['artist_names'].unique().tolist()\n    return names\n</code></pre>"},{"location":"dataset/#src.pages.dataset.Monitor.access_feature_definitions","title":"<code>access_feature_definitions()</code>","text":"<p>Method allows for the <code>data/feature_def</code> file to be read-in providing feature definitions</p> <p>Returns:</p> Type Description <code>str</code> <p>Feature definitions</p> Source code in <code>src/pages/dataset.py</code> <pre><code>def access_feature_definitions(self):\n    \"\"\"Method allows for the `data/feature_def` file to be read-in providing feature definitions\n\n    Returns:\n        (str): Feature definitions\n    \"\"\"\n    def_file = os.path.join(self.root_path, 'data', 'feature_def.txt')\n    with open(def_file, 'r') as file:\n        contents = file.read()\n        return contents\n</code></pre>"},{"location":"dataset/#src.pages.dataset.Monitor.access_specific_features","title":"<code>access_specific_features(selection, sample=True)</code>","text":"<p>Method allows for access to specific acoustic features in the tracks dataset</p> <p>Parameters:</p> Name Type Description Default <code>selection</code> <code>list</code> <p>A list of acoustic features</p> required <code>sample</code> <code>bool</code> <p>If True, the features must be sampled. If False, the feature is not sampled.</p> <code>True</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A dataframe containing the specified feature.</p> Source code in <code>src/pages/dataset.py</code> <pre><code>def access_specific_features(self, selection: list, sample=True):\n    \"\"\"Method allows for access to specific acoustic features in the tracks dataset\n\n    Args:\n        selection (list): A list of acoustic features\n        sample (bool): If True, the features must be sampled. If False, the feature is not sampled.\n\n    Returns:\n        (DataFrame): A dataframe containing the specified feature.\n    \"\"\"\n    if sample:\n        track_sample = self.tracks.sample(frac=0.1, random_state=2)\n    else:\n        track_sample = self.tracks\n    return track_sample[selection]\n</code></pre>"},{"location":"dataset/#src.pages.dataset.Monitor.determine_date_range","title":"<code>determine_date_range()</code>","text":"<p>Method determines the date range in the dataset history file.</p> <p>Returns:</p> Name Type Description <code>start</code> <code>datetime</code> <p>The earliest date available in the dataset history file.</p> <code>end</code> <code>datetime</code> <p>The latest date available in the dataset history file.</p> Source code in <code>src/pages/dataset.py</code> <pre><code>def determine_date_range(self):\n    \"\"\"Method determines the date range in the dataset history file.\n\n    Returns:\n        start (datetime): The earliest date available in the dataset history file.\n        end (datetime): The latest date available in the dataset history file.\n    \"\"\"\n    return self.history['date'].min(), self.history['date'].max()\n</code></pre>"},{"location":"dataset/#src.pages.dataset.artist_matching","title":"<code>artist_matching(artists)</code>","text":"<p>Method performs a regex search against all artists in the database, using the artists search query.</p> <p>Parameters:</p> Name Type Description Default <code>artists</code> <code>str</code> <p>A search pattern of artists names (expect the string to be of format name,name,name with no whitespaces)</p> required <p>Returns:     (list): A list of matching artist names which the user can then select from.</p> Source code in <code>src/pages/dataset.py</code> <pre><code>def artist_matching(artists: str):\n    \"\"\"Method performs a regex search against all artists in the database, using the artists search query.\n\n    Args:\n        artists (str): A search pattern of artists names (expect the string to be of format name,name,name with no whitespaces)\n    Returns:\n        (list): A list of matching artist names which the user can then select from.\n    \"\"\"\n    options = []\n    artist_names = re.split(r',|\\|', artists)\n    pattern = re.compile(fr\"\\b(?:{'|'.join(map(re.escape, artist_names))})\\b\", re.IGNORECASE)\n    names = st.session_state.monitor.access_artist_names()\n    for name in names:\n        matches = re.findall(pattern, name)\n        if matches:\n            options.append(name)\n    return options\n</code></pre>"},{"location":"dataset/#src.pages.dataset.create_feature_selection","title":"<code>create_feature_selection(maximum=12)</code>","text":"<p>Method creates a streamlit multiselection capability, in which a user can select acoustic featurs to be visualized</p> <p>Parameters:</p> Name Type Description Default <code>maximum</code> <code>int</code> <p>The maximum number of features that can be selected.</p> <code>12</code> <p>Returns:</p> Type Description <code>list</code> <p>A list of selected feature names</p> Source code in <code>src/pages/dataset.py</code> <pre><code>def create_feature_selection(maximum=12):\n    \"\"\"Method creates a streamlit multiselection capability, in which a user can select acoustic featurs to be visualized\n\n    Args:\n        maximum (int): The maximum number of features that can be selected.\n\n    Returns:\n        (list): A list of selected feature names\n    \"\"\"\n    options = ['artist_pop', 'track_pop', 'danceability', 'energy', 'loudness', 'speechiness', 'acousticness',\n               'instrumentalness', 'liveness', 'valences', 'durations_ms', 'tempos']\n    selected_option = st.multiselect('Select features', options, default='loudness', max_selections=maximum)\n    return selected_option\n</code></pre>"},{"location":"dataset/#src.pages.dataset.datasets_download_section","title":"<code>datasets_download_section()</code>","text":"<p>Method prepares the <code>tracks.csv</code> and the <code>dataset_growth.csv</code> files for download.</p> Source code in <code>src/pages/dataset.py</code> <pre><code>def datasets_download_section():\n    \"\"\"Method prepares the `tracks.csv` and the `dataset_growth.csv` files for download.\"\"\"\n    # Tracks download\n    with open(st.session_state.monitor.track_path, 'rb') as file:\n        data = file.read()\n\n    st.download_button(\n        label='Click to download tracks dataset',\n        data=data,\n        file_name='tracks.csv',\n        key='download_dataset'\n    )\n\n    # Growth download\n    with open(st.session_state.monitor.hist_path, 'rb') as history_file:\n        data_hist = history_file.read()\n\n    st.download_button(\n        label='Click to download tracks growth dataset',\n        data=data_hist,\n        file_name='dataset_growth.csv',\n        key='download_dataset_growth'\n    )\n</code></pre>"},{"location":"dataset/#src.pages.dataset.date_sliders","title":"<code>date_sliders(start, end)</code>","text":"<p>Method creates the dataset growth date sliders, in order to determine the start and end date for visualizatin</p> <p>Parameters:</p> Name Type Description Default <code>start</code> <code>datetime</code> <p>Earliest available date in the history dataset</p> required <code>end</code> <code>dadtetime</code> <p>Latest available date in the history dataset.</p> required <p>Returns:</p> Name Type Description <code>start_date</code> <code>datetime</code> <p>The slider selected start date</p> <code>end_date</code> <code>datetime</code> <p>The slider selected end date</p> Source code in <code>src/pages/dataset.py</code> <pre><code>def date_sliders(start, end):\n    \"\"\"Method creates the dataset growth date sliders, in order to determine the start and end date for visualizatin\n\n    Args:\n        start (datetime): Earliest available date in the history dataset\n        end (dadtetime): Latest available date in the history dataset.\n\n    Returns:\n        start_date (datetime): The slider selected start date\n        end_date (datetime): The slider selected end date\n    \"\"\"\n    start_date = st.slider(label='Select start date',\n                           min_value=start.to_pydatetime(),\n                           max_value=end.to_pydatetime())\n    end_date = st.slider(label='Select end date',\n                         min_value=start.to_pydatetime(),\n                         max_value=end.to_pydatetime(),\n                         value=max_date.to_pydatetime())\n    return start_date, end_date\n</code></pre>"},{"location":"dataset/#src.pages.dataset.generate_artist_comparison","title":"<code>generate_artist_comparison(selection, artist_filter)</code>","text":"<p>Method generates a swarmplot enabling artist comparison across various acoustic features. Args:     selection (list): A list of features, such that their distributions will be visually compared.     artist_filter (list): A filter of artist names, if the distributions</p> <p>Returns:</p> Type Description <code>PyPlot Figure</code> <p>A swarmplot showing comparable artist acoustic features</p> Source code in <code>src/pages/dataset.py</code> <pre><code>def generate_artist_comparison(selection: list, artist_filter: list):\n    \"\"\"Method generates a swarmplot enabling artist comparison across various acoustic features.\n    Args:\n        selection (list): A list of features, such that their distributions will be visually compared.\n        artist_filter (list): A filter of artist names, if the distributions\n\n    Returns:\n        (PyPlot Figure): A swarmplot showing comparable artist acoustic features\n    \"\"\"\n    selection.append('artist_names')\n    df = st.session_state.monitor.access_specific_features(selection, sample=False)\n    df = df[df['artist_names'].isin(artist_filter)]  # Filter df to keep arist only tracks\n    selection.remove('artist_names')\n\n    scaler = MinMaxScaler(feature_range=(-1, 1))  # Normalize the data between [-1, 1] for visual purposes\n    columns = selection\n    df[columns] = scaler.fit_transform(df[columns])\n\n    sns.set()\n    fig, ax = plt.subplots(figsize=(10, 6))\n    h = sns.swarmplot(data=df, x=selection[0], y='artist_names', hue='artist_names', palette=\"Set2\", legend=False)\n    return fig\n</code></pre>"},{"location":"dataset/#src.pages.dataset.generate_distribution","title":"<code>generate_distribution(selection)</code>","text":"<p>Method generates feature distibutions to allow feature comparisons.</p> <p>Note, all features are normalized within the same range [-1, 1] for comparative visualization purposes.</p> <p>Parameters:</p> Name Type Description Default <code>selection</code> <code>list</code> <p>A list of features, such that their distributions will be visually compared.</p> required <p>Returns:</p> Type Description <code>PyPlot Figure</code> <p>A figure showcasing the various acoustic feature distributions</p> Source code in <code>src/pages/dataset.py</code> <pre><code>def generate_distribution(selection: list):\n    \"\"\"Method generates feature distibutions to allow feature comparisons.\n\n    Note, all features are normalized within the same range [-1, 1] for comparative visualization purposes.\n\n    Args:\n        selection (list): A list of features, such that their distributions will be visually compared.\n\n    Returns:\n        (PyPlot Figure): A figure showcasing the various acoustic feature distributions\n    \"\"\"\n    df = st.session_state.monitor.access_specific_features(selection)\n\n    scaler = MinMaxScaler(feature_range=(-1, 1))  # Normalize the data between [-1, 1] for visual purposes\n    columns = selection\n    df[columns] = scaler.fit_transform(df[columns])\n\n    sns.set()\n    fig, ax = plt.subplots(figsize=(10, 6))\n\n    for feature in selection:  # Overlay the feature distributions\n        h = sns.kdeplot(data=df, x=feature, label=feature, fill=True)\n    ax.set_title('Acoustic Feature Distribution')\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Density')\n    ax.legend()\n\n    return fig\n</code></pre>"},{"location":"dataset/#src.pages.dataset.generate_growth_plot","title":"<code>generate_growth_plot(start, end)</code>","text":"<p>This method generates a line plot showcasing the number of unique tracks in the dataset over time, showcasing its growth</p> <p>Parameters:</p> Name Type Description Default <code>start</code> <code>datetime</code> <p>The starting datetime to visualize the dataset growth</p> required <code>end</code> <code>datetime</code> <p>The end datetime to visualize the dataset growth</p> required <p>Returns:</p> Type Description <code>PyPlot Figure</code> <p>A line plot figure showcasing the dataset growth over time</p> Source code in <code>src/pages/dataset.py</code> <pre><code>@st.cache_resource\ndef generate_growth_plot(start, end):\n    \"\"\"This method generates a line plot showcasing the number of unique tracks in the dataset over time, showcasing its growth\n\n    Args:\n        start (datetime): The starting datetime to visualize the dataset growth\n        end (datetime): The end datetime to visualize the dataset growth\n\n    Returns:\n        (PyPlot Figure): A line plot figure showcasing the dataset growth over time\n    \"\"\"\n    df = st.session_state.monitor.history\n    history_filtered = df[(df['date'] &gt;= start) &amp; (df['date'] &lt;= end)]\n\n    sns.set()\n    fig, axes = plt.subplots(1, 1, figsize=(12, 6))\n    h = sns.lineplot(data=history_filtered, x='date', y='track_count', color='black')\n    axes.fill_between(history_filtered['date'], history_filtered['track_count'], alpha=0.2, color='red')\n\n    axes.set_xlabel('Date')\n    axes.set_ylabel('Track Count')\n\n    return fig\n</code></pre>"},{"location":"dataset/#src.pages.dataset.generate_pair_plot","title":"<code>generate_pair_plot()</code>","text":"<p>This function generates a pair plot showcasing the relationship between acoustic analysis features of the tracks in the dataset.</p> <p>Returns:</p> Type Description <code>PyPlot Figure</code> <p>A pyplot figure showcasing the acoustic feature relationships</p> Source code in <code>src/pages/dataset.py</code> <pre><code>@st.cache_resource\ndef generate_pair_plot():\n    \"\"\"This function generates a pair plot showcasing the relationship between acoustic analysis features of the tracks in the dataset.\n\n    Returns:\n        (PyPlot Figure): A pyplot figure showcasing the acoustic feature relationships\n    \"\"\"\n    df = st.session_state.monitor.access_acoustic_sample_features()\n\n    sns.set()\n    g = sns.pairplot(df, diag_kind='kde')\n    return g.fig\n</code></pre>"},{"location":"pipeline/","title":"Pipeline","text":""},{"location":"pipeline/#cosine-pipeline","title":"Cosine Pipeline","text":"<p>This file provides the transformation pipeline from raw tracks to a set of track features ready for use by the Cosine Similarity class.</p> <p>Pipeline inherits from the Pipeline Interface.</p>"},{"location":"pipeline/#pipeline-documentation","title":"Pipeline Documentation","text":""},{"location":"pipeline/#src.pipeline.CosinePipeline","title":"<code>CosinePipeline</code>","text":"<p>             Bases: <code>Pipeline</code></p> <p>This class serves as the transformation pipeline from raw data to usable features, for the similarity calculation carried out by the Cosine Similarity class.</p> Source code in <code>src/pipeline.py</code> <pre><code>class CosinePipeline(Pipeline):\n    \"\"\"This class serves as the transformation pipeline from raw data to usable features, for the similarity calculation\n    carried out by the Cosine Similarity class.\"\"\"\n\n    @staticmethod\n    def select_columns(df):\n        \"\"\"This method selects the columns to be included within feature calculations.\n\n        This method, allows for the ignoring/ removal of peripheral or uneeded columns.\n\n        Args:\n            df (DataFrame): The dataframe containing the raw data from the `data/tracks.csv` file\n\n        Returns:\n            (DataFrame): The modified dataframe object containing only the select columns.\n        \"\"\"\n        df = df[['uris', 'artist_pop',\n                 'artist_genres', 'track_pop', 'danceability', 'energy',\n                 'keys', 'loudness', 'modes', 'speechiness', 'acousticness',\n                 'instrumentalness', 'liveness', 'valences', 'tempos', 'durations_ms', 'time_signatures']]\n        return df\n\n    @staticmethod\n    def ohe_prep(df, column):\n        \"\"\"This method performs a One-Hot-Encoding (OHE) on a specified column\n\n        Args:\n            df (DataFrame): The dataframe containing the tracks information, now being processes by the pipeline.\n            column (str): The name of the column on which the OHE transformation should be performed.\n        \"\"\"\n        df_encoded = pd.get_dummies(df, columns=[column], dtype=int)\n        return df_encoded\n\n    @staticmethod\n    def tfidf_transformation(df_parm):\n        \"\"\"This method performs the term frequency\u2013inverse document frequency (tfidf) transformation on the `artist genre` column.\n\n        Note, more information on tfidf transformation can be found here: https://www.geeksforgeeks.org/understanding-tf-idf-term-frequency-inverse-document-frequency/\n\n        Args:\n            df_parm (DataFrame): The dataframe containing the tracks information, now undergoing tfidg transfromation\n\n        Returns:\n            (DataFrame): The transformed dataframe containing the results of the tfidf transfromation.\n        \"\"\"\n        tf = TfidfVectorizer(analyzer='word', ngram_range=(1, 1), min_df=0.0, max_features=50)\n        tfidf_matrix = tf.fit_transform(df_parm['artist_genres'])\n\n        genre_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tf.get_feature_names_out())\n        genre_df.columns = ['genre' + \"|\" + i for i in genre_df.columns]\n\n        df_parm = df_parm.drop(columns=['artist_genres'])\n\n        combined_df = pd.concat([df_parm.reset_index(drop=True), genre_df.reset_index(drop=True)], axis=1)\n        return combined_df\n\n    @staticmethod\n    def data_pipeline(df):\n        \"\"\"This method enacts the transformation pipeline to produce a set of track features.\n\n        This pipeline makes use of the following transformation methods:\n        - One-hot_encoding\n        - TFIDF transformation\n        - Min-Max Scaling of numerical values\n\n        It is essential to note that all features are normalized between [0, 1]\n\n        Args:\n            df (DataFrame): The dataframe containing the raw data from the `data/tracks.csv` file\n\n        Returns:\n              (DataFrame): A dataframe containing all track features\n        \"\"\"\n\n        df_pipe = CosinePipeline.select_columns(df)\n\n        # Perform OHE\n        df_pipe = CosinePipeline.ohe_prep(df_pipe, 'modes')\n        df_pipe = CosinePipeline.ohe_prep(df_pipe, 'keys')\n        df_pipe = CosinePipeline.ohe_prep(df_pipe, 'time_signatures')\n\n        # Normalize popularity values\n        scaler = MinMaxScaler(feature_range=(0, 1))\n        columns = ['danceability', 'energy', 'loudness', 'speechiness', 'acousticness', 'instrumentalness', 'liveness', 'valences', 'durations_ms', 'tempos']\n        df_pipe[columns] = scaler.fit_transform(df_pipe[columns])\n\n        # Perform TFID vectorization on genres\n        df_pipe = CosinePipeline.tfidf_transformation(df_parm=df_pipe)\n\n        df_pipe = df_pipe.set_index(keys='uris', drop=True)\n\n        return df_pipe\n\n    @staticmethod\n    def unique_tracks(df, df_target):\n        \"\"\"Method ensures that df (tracks dataframe) does not contain the same tracks as the playlist (df_target)\n\n        Args:\n            df (DataFrame): The dataframe containing the tracks dataset.\n            df_target (DataFrame): The dataframe containing the tracks from the playlist\n\n        Returns:\n            (DataFrame): The tracks dataframe containing none of the same tracks as in the playlist.\n        \"\"\"\n        df = df.drop(df_target['uris'], errors='ignore')\n        return df\n\n    @staticmethod\n    def extract_target(df, df_target):\n        \"\"\"This method allows for the extraction of tracks in the playlist from the tracks dataset.\n        Note, for this method to work, the track uris should be the index in the tracks dataframe (df).\n\n        This method is largely used once the tracks dataset has been transformed into a set of features,\n        and the playlist track features are required to be extracted.\n\n        Args:\n            df (DataFrame): The dataframe containing the tracks dataset.\n            df_target (DataFrame): The dataframe containing the tracks from the playlist\n\n        Returns:\n            (DataFrame): A resulting dataframe containing only the tracks from the playlist that were in the tracks dataset.\n        \"\"\"\n        target_uris = df_target['uris'].tolist()\n        return df[df.index.isin(target_uris)]\n</code></pre>"},{"location":"pipeline/#src.pipeline.CosinePipeline.data_pipeline","title":"<code>data_pipeline(df)</code>  <code>staticmethod</code>","text":"<p>This method enacts the transformation pipeline to produce a set of track features.</p> <p>This pipeline makes use of the following transformation methods: - One-hot_encoding - TFIDF transformation - Min-Max Scaling of numerical values</p> <p>It is essential to note that all features are normalized between [0, 1]</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The dataframe containing the raw data from the <code>data/tracks.csv</code> file</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A dataframe containing all track features</p> Source code in <code>src/pipeline.py</code> <pre><code>@staticmethod\ndef data_pipeline(df):\n    \"\"\"This method enacts the transformation pipeline to produce a set of track features.\n\n    This pipeline makes use of the following transformation methods:\n    - One-hot_encoding\n    - TFIDF transformation\n    - Min-Max Scaling of numerical values\n\n    It is essential to note that all features are normalized between [0, 1]\n\n    Args:\n        df (DataFrame): The dataframe containing the raw data from the `data/tracks.csv` file\n\n    Returns:\n          (DataFrame): A dataframe containing all track features\n    \"\"\"\n\n    df_pipe = CosinePipeline.select_columns(df)\n\n    # Perform OHE\n    df_pipe = CosinePipeline.ohe_prep(df_pipe, 'modes')\n    df_pipe = CosinePipeline.ohe_prep(df_pipe, 'keys')\n    df_pipe = CosinePipeline.ohe_prep(df_pipe, 'time_signatures')\n\n    # Normalize popularity values\n    scaler = MinMaxScaler(feature_range=(0, 1))\n    columns = ['danceability', 'energy', 'loudness', 'speechiness', 'acousticness', 'instrumentalness', 'liveness', 'valences', 'durations_ms', 'tempos']\n    df_pipe[columns] = scaler.fit_transform(df_pipe[columns])\n\n    # Perform TFID vectorization on genres\n    df_pipe = CosinePipeline.tfidf_transformation(df_parm=df_pipe)\n\n    df_pipe = df_pipe.set_index(keys='uris', drop=True)\n\n    return df_pipe\n</code></pre>"},{"location":"pipeline/#src.pipeline.CosinePipeline.extract_target","title":"<code>extract_target(df, df_target)</code>  <code>staticmethod</code>","text":"<p>This method allows for the extraction of tracks in the playlist from the tracks dataset. Note, for this method to work, the track uris should be the index in the tracks dataframe (df).</p> <p>This method is largely used once the tracks dataset has been transformed into a set of features, and the playlist track features are required to be extracted.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The dataframe containing the tracks dataset.</p> required <code>df_target</code> <code>DataFrame</code> <p>The dataframe containing the tracks from the playlist</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A resulting dataframe containing only the tracks from the playlist that were in the tracks dataset.</p> Source code in <code>src/pipeline.py</code> <pre><code>@staticmethod\ndef extract_target(df, df_target):\n    \"\"\"This method allows for the extraction of tracks in the playlist from the tracks dataset.\n    Note, for this method to work, the track uris should be the index in the tracks dataframe (df).\n\n    This method is largely used once the tracks dataset has been transformed into a set of features,\n    and the playlist track features are required to be extracted.\n\n    Args:\n        df (DataFrame): The dataframe containing the tracks dataset.\n        df_target (DataFrame): The dataframe containing the tracks from the playlist\n\n    Returns:\n        (DataFrame): A resulting dataframe containing only the tracks from the playlist that were in the tracks dataset.\n    \"\"\"\n    target_uris = df_target['uris'].tolist()\n    return df[df.index.isin(target_uris)]\n</code></pre>"},{"location":"pipeline/#src.pipeline.CosinePipeline.ohe_prep","title":"<code>ohe_prep(df, column)</code>  <code>staticmethod</code>","text":"<p>This method performs a One-Hot-Encoding (OHE) on a specified column</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The dataframe containing the tracks information, now being processes by the pipeline.</p> required <code>column</code> <code>str</code> <p>The name of the column on which the OHE transformation should be performed.</p> required Source code in <code>src/pipeline.py</code> <pre><code>@staticmethod\ndef ohe_prep(df, column):\n    \"\"\"This method performs a One-Hot-Encoding (OHE) on a specified column\n\n    Args:\n        df (DataFrame): The dataframe containing the tracks information, now being processes by the pipeline.\n        column (str): The name of the column on which the OHE transformation should be performed.\n    \"\"\"\n    df_encoded = pd.get_dummies(df, columns=[column], dtype=int)\n    return df_encoded\n</code></pre>"},{"location":"pipeline/#src.pipeline.CosinePipeline.select_columns","title":"<code>select_columns(df)</code>  <code>staticmethod</code>","text":"<p>This method selects the columns to be included within feature calculations.</p> <p>This method, allows for the ignoring/ removal of peripheral or uneeded columns.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The dataframe containing the raw data from the <code>data/tracks.csv</code> file</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>The modified dataframe object containing only the select columns.</p> Source code in <code>src/pipeline.py</code> <pre><code>@staticmethod\ndef select_columns(df):\n    \"\"\"This method selects the columns to be included within feature calculations.\n\n    This method, allows for the ignoring/ removal of peripheral or uneeded columns.\n\n    Args:\n        df (DataFrame): The dataframe containing the raw data from the `data/tracks.csv` file\n\n    Returns:\n        (DataFrame): The modified dataframe object containing only the select columns.\n    \"\"\"\n    df = df[['uris', 'artist_pop',\n             'artist_genres', 'track_pop', 'danceability', 'energy',\n             'keys', 'loudness', 'modes', 'speechiness', 'acousticness',\n             'instrumentalness', 'liveness', 'valences', 'tempos', 'durations_ms', 'time_signatures']]\n    return df\n</code></pre>"},{"location":"pipeline/#src.pipeline.CosinePipeline.tfidf_transformation","title":"<code>tfidf_transformation(df_parm)</code>  <code>staticmethod</code>","text":"<p>This method performs the term frequency\u2013inverse document frequency (tfidf) transformation on the <code>artist genre</code> column.</p> <p>Note, more information on tfidf transformation can be found here: https://www.geeksforgeeks.org/understanding-tf-idf-term-frequency-inverse-document-frequency/</p> <p>Parameters:</p> Name Type Description Default <code>df_parm</code> <code>DataFrame</code> <p>The dataframe containing the tracks information, now undergoing tfidg transfromation</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>The transformed dataframe containing the results of the tfidf transfromation.</p> Source code in <code>src/pipeline.py</code> <pre><code>@staticmethod\ndef tfidf_transformation(df_parm):\n    \"\"\"This method performs the term frequency\u2013inverse document frequency (tfidf) transformation on the `artist genre` column.\n\n    Note, more information on tfidf transformation can be found here: https://www.geeksforgeeks.org/understanding-tf-idf-term-frequency-inverse-document-frequency/\n\n    Args:\n        df_parm (DataFrame): The dataframe containing the tracks information, now undergoing tfidg transfromation\n\n    Returns:\n        (DataFrame): The transformed dataframe containing the results of the tfidf transfromation.\n    \"\"\"\n    tf = TfidfVectorizer(analyzer='word', ngram_range=(1, 1), min_df=0.0, max_features=50)\n    tfidf_matrix = tf.fit_transform(df_parm['artist_genres'])\n\n    genre_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tf.get_feature_names_out())\n    genre_df.columns = ['genre' + \"|\" + i for i in genre_df.columns]\n\n    df_parm = df_parm.drop(columns=['artist_genres'])\n\n    combined_df = pd.concat([df_parm.reset_index(drop=True), genre_df.reset_index(drop=True)], axis=1)\n    return combined_df\n</code></pre>"},{"location":"pipeline/#src.pipeline.CosinePipeline.unique_tracks","title":"<code>unique_tracks(df, df_target)</code>  <code>staticmethod</code>","text":"<p>Method ensures that df (tracks dataframe) does not contain the same tracks as the playlist (df_target)</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The dataframe containing the tracks dataset.</p> required <code>df_target</code> <code>DataFrame</code> <p>The dataframe containing the tracks from the playlist</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>The tracks dataframe containing none of the same tracks as in the playlist.</p> Source code in <code>src/pipeline.py</code> <pre><code>@staticmethod\ndef unique_tracks(df, df_target):\n    \"\"\"Method ensures that df (tracks dataframe) does not contain the same tracks as the playlist (df_target)\n\n    Args:\n        df (DataFrame): The dataframe containing the tracks dataset.\n        df_target (DataFrame): The dataframe containing the tracks from the playlist\n\n    Returns:\n        (DataFrame): The tracks dataframe containing none of the same tracks as in the playlist.\n    \"\"\"\n    df = df.drop(df_target['uris'], errors='ignore')\n    return df\n</code></pre>"},{"location":"pipeline_interface/","title":"Pipeline interface","text":""},{"location":"pipeline_interface/#pipeline-interface","title":"Pipeline (Interface)","text":"<p>This file provides the framework for the transformation pipeline from raw track data located in <code>data/tracks.csv</code> through to features used for similarity calculations. Each pipeline must inherent from the pipeline interface. </p> <p>Note: Different pipelines may result in varied features, hence the need for a pipeline interface, such that there exists a common method for the front-end to make use of.</p>"},{"location":"pipeline_interface/#pipeline-interface-documentation","title":"Pipeline Interface Documentation","text":""},{"location":"pipeline_interface/#src.pipeline_interface.Pipeline","title":"<code>Pipeline</code>","text":"<p>             Bases: <code>ABC</code></p> <p>This class serves as transformation pipeline from Spotify tracks raw data to track features</p> <p>Note, the tracks database is stored in the <code>data/tracks.csv</code> file.</p> Source code in <code>src/pipeline_interface.py</code> <pre><code>class Pipeline(ABC):\n    \"\"\"This class serves as transformation pipeline from Spotify tracks raw data to track features\n\n    Note, the tracks database is stored in the `data/tracks.csv` file.\n    \"\"\"\n    @staticmethod\n    @abstractmethod\n    def data_pipeline(df):\n        \"\"\"This method enacts the transformation pipeline to produce a set of track features.\n        It is expected for the playlist to return a pandas DataFrame containing all features.\n\n        Args:\n            df (DataFrame): The dataframe containing the raw data from the `data/tracks.csv` file\n        \"\"\"\n        pass\n</code></pre>"},{"location":"pipeline_interface/#src.pipeline_interface.Pipeline.data_pipeline","title":"<code>data_pipeline(df)</code>  <code>abstractmethod</code> <code>staticmethod</code>","text":"<p>This method enacts the transformation pipeline to produce a set of track features. It is expected for the playlist to return a pandas DataFrame containing all features.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The dataframe containing the raw data from the <code>data/tracks.csv</code> file</p> required Source code in <code>src/pipeline_interface.py</code> <pre><code>@staticmethod\n@abstractmethod\ndef data_pipeline(df):\n    \"\"\"This method enacts the transformation pipeline to produce a set of track features.\n    It is expected for the playlist to return a pandas DataFrame containing all features.\n\n    Args:\n        df (DataFrame): The dataframe containing the raw data from the `data/tracks.csv` file\n    \"\"\"\n    pass\n</code></pre>"},{"location":"recommender/","title":"Recommender","text":""},{"location":"recommender/#recommender-page","title":"Recommender Page","text":"<p>The recommender page (Home Page) provides the User Interface (UI) structure and process behind the recommendation system.</p>"},{"location":"recommender/#recommender-page-documentation","title":"Recommender Page Documentation","text":""},{"location":"recommender/#src.recommender.access_tracks","title":"<code>access_tracks()</code>","text":"<p>Method enables access to saved track information.</p> <p>Returns:</p> Name Type Description <code>df</code> <code>DataFrame</code> <p>The dataframe containing all feature information of saved tracks</p> Source code in <code>src/recommender.py</code> <pre><code>def access_tracks():\n    \"\"\"Method enables access to saved track information.\n\n    Returns:\n        df (DataFrame): The dataframe containing all feature information of saved tracks\n    \"\"\"\n    root_path = os.path.abspath(os.path.dirname(os.path.dirname(__file__)))\n    file_path = os.path.join(root_path, 'data', 'tracks.csv')\n    df = pd.read_csv(file_path, index_col=0)  # Read in stored tracks dataframe\n    return df\n</code></pre>"},{"location":"recommender/#src.recommender.create_feature_weighting","title":"<code>create_feature_weighting(maximum=12)</code>","text":"<p>Creates a streamlit dropdown menu, providing a selection of features that the user can select to weight.</p> <p>Parameters:</p> Name Type Description Default <code>maximum</code> <code>int</code> <p>The maximum number of features you can choose for weighting.</p> <code>12</code> <p>Returns:</p> Type Description <code>list</code> <p>A list of selected feature names.</p> Source code in <code>src/recommender.py</code> <pre><code>def create_feature_weighting(maximum=12):\n    \"\"\"Creates a streamlit dropdown menu, providing a selection of features that the user can select to weight.\n\n    Args:\n        maximum (int): The maximum number of features you can choose for weighting.\n\n    Returns:\n        (list): A list of selected feature names.\n    \"\"\"\n    options = ['artist_pop', 'track_pop', 'danceability', 'energy', 'loudness', 'speechiness', 'acousticness',\n               'instrumentalness', 'liveness', 'valences', 'tempos', 'durations_ms', 'tempos']\n    selected_option = st.multiselect('Select features to weight', options, max_selections=maximum)\n    return selected_option\n</code></pre>"},{"location":"recommender/#src.recommender.display_spotify_recommendations","title":"<code>display_spotify_recommendations()</code>","text":"<p>Method deals with displaying the Spotify recommendations in the form of Spotify iFrames for each recommendation.</p> <p>This section is composed of two columns, with 30 recommendations split evenly between them creating a 15 x 2 table.</p> Source code in <code>src/recommender.py</code> <pre><code>def display_spotify_recommendations():\n    \"\"\"Method deals with displaying the Spotify recommendations in the form of Spotify iFrames for each recommendation.\n\n    This section is composed of two columns, with 30 recommendations split evenly between them creating a 15 x 2 table.\n    \"\"\"\n    st.markdown(\"#### Recommended Tracks\")\n\n    col1, col2 = st.columns(2)\n    count = 0\n    rec_df = st.session_state.similarity.get_top_n(30)\n    for index, row in rec_df.iterrows():\n        spotify_uri = row['uris']\n        embed_code = f'&lt;iframe src=\"https://open.spotify.com/embed/track/{spotify_uri.split(\":\")[-1]}\" ' \\\n                     'width=\"300\" height=\"80\" frameborder=\"0\" allowtransparency=\"true\" allow=\"encrypted-media\"&gt;&lt;/iframe&gt;'\n\n        if count % 2 == 0:\n            with col1:\n                st.markdown(embed_code, unsafe_allow_html=True)\n        else:\n            with col2:\n                st.markdown(embed_code, unsafe_allow_html=True)\n        count += 1\n</code></pre>"},{"location":"recommender/#src.recommender.feature_def_section","title":"<code>feature_def_section()</code>","text":"<p>This method creates the features section in the Streamlit app.</p> <p>Specifically, it creates the feature definitions drop down information.</p> Source code in <code>src/recommender.py</code> <pre><code>def feature_def_section():\n    \"\"\"This method creates the features section in the Streamlit app.\n\n    Specifically, it creates the feature definitions drop down information.\n    \"\"\"\n    st.markdown(\n        '**For advanced recommender personalization please select the features to be weighted (have more importance) to '\n        'your search**')\n\n    with st.expander('Feature Definitions'):\n        feature_defs = retrieve_feature_defs()\n        for definition in feature_defs.split('\\\\n'):\n            st.markdown(definition)\n</code></pre>"},{"location":"recommender/#src.recommender.playlist_submission","title":"<code>playlist_submission()</code>","text":"<p>Method handles the process that follows the clicking of the <code>submit</code> button</p> <p>The process is as follows: - The given playlist tracks are retrieved. - The tracks dataset is read in - Similarity is calculated - Streamlit session states are updated</p> Source code in <code>src/recommender.py</code> <pre><code>def playlist_submission():\n    \"\"\"Method handles the process that follows the clicking of the `submit` button\n\n    The process is as follows:\n    - The given playlist tracks are retrieved.\n    - The tracks dataset is read in\n    - Similarity is calculated\n    - Streamlit session states are updated\n    \"\"\"\n    df_playlist = retrieve_target_playlist(playlist_url, playlist_name)\n    df_tracks = access_tracks()\n\n    st.session_state.similarity = TracksCosineSimilarity(df_playlist, df_tracks, st.session_state.weighted_features)\n    st.session_state.similarity.calculate_similarity()\n    update_tracking(df_tracks)\n\n    st.session_state.playlist_links.append(playlist_url)\n    st.session_state.playlist_names.append(playlist_name)\n</code></pre>"},{"location":"recommender/#src.recommender.playlist_to_df","title":"<code>playlist_to_df(playlist)</code>","text":"<p>Method transforms playlist information in dictionary form to a playlist dataframe Args:     playlist (dict): Playlist information in a dictionary, keys are features and values are a list.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A playlist dataframe.</p> Source code in <code>src/recommender.py</code> <pre><code>def playlist_to_df(playlist: dict):\n    \"\"\"Method transforms playlist information in dictionary form to a playlist dataframe\n    Args:\n        playlist (dict): Playlist information in a dictionary, keys are features and values are a list.\n\n    Returns:\n        (DataFrame): A playlist dataframe.\n    \"\"\"\n    target_df = pd.DataFrame.from_dict(playlist)\n    return target_df\n</code></pre>"},{"location":"recommender/#src.recommender.retrieve_feature_defs","title":"<code>retrieve_feature_defs()</code>","text":"<p>Method retrieves the feature definitions from the <code>data/feature_def.txt</code> file.</p> <p>Returns:</p> Type Description <code>str</code> <p>The contents of the <code>feature_def.txt</code> file.</p> Source code in <code>src/recommender.py</code> <pre><code>def retrieve_feature_defs():\n    \"\"\"Method retrieves the feature definitions from the `data/feature_def.txt` file.\n\n    Returns:\n        (str): The contents of the `feature_def.txt` file.\n    \"\"\"\n    root_path = os.path.abspath(os.path.dirname(os.path.dirname(__file__)))\n    file_path = os.path.join(root_path, 'data', 'feature_def.txt')\n    with open(file_path, 'r') as file:\n        contents = file.read()\n        return contents\n</code></pre>"},{"location":"recommender/#src.recommender.retrieve_target_playlist","title":"<code>retrieve_target_playlist(url, name)</code>","text":"<p>This method gathers all the playlist song features and merges this data into the tracks dataset.</p> <pre><code>Note: credentials are stored using Streamlit secrets keeper\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The url for the spotify playlist</p> required <code>name</code> <code>str</code> <p>The name of the spotify playlist</p> required <p>Returns:     playlist (DataFrame): The playlist features as a DataFrame</p> Source code in <code>src/recommender.py</code> <pre><code>def retrieve_target_playlist(url: str, name: str):\n    \"\"\" This method gathers all the playlist song features and merges this data into the tracks dataset.\n\n        Note: credentials are stored using Streamlit secrets keeper\n\n    Args:\n        url (str): The url for the spotify playlist\n        name (str): The name of the spotify playlist\n    Returns:\n        playlist (DataFrame): The playlist features as a DataFrame\n    \"\"\"\n    client_credentials_manager = SpotifyClientCredentials(client_id=st.secrets['CLIENT_ID'],\n                                                          client_secret=st.secrets[\n                                                              'CLIENT_SECRET'])  # Set up Spotify Credentials\n    sp = spotipy.Spotify(client_credentials_manager=client_credentials_manager)\n    playlist = target_playlist_extraction(sp, url, name)  # Generate target playlist dataframe\n    save_data(playlist)  # Save the playlist tracks into the larger tracks dataset\n    playlist_df = playlist_to_df(playlist)\n    return playlist_df\n</code></pre>"},{"location":"recommender/#src.recommender.search_history_section","title":"<code>search_history_section()</code>","text":"<p>This method creates the search history section of the Streamlit application.</p> <p>This section provides a drop-down that displays the search history in the from playlist name | playlist url</p> Source code in <code>src/recommender.py</code> <pre><code>def search_history_section():\n    \"\"\"This method creates the search history section of the Streamlit application.\n\n    This section provides a drop-down that displays the search history in the from\n    playlist name | playlist url\n    \"\"\"\n    with st.expander(\"Search History\", expanded=False):\n        if len(st.session_state.playlist_links) == 0:\n            st.markdown('No search history yet...')\n        else:\n            for name, url in zip(st.session_state.playlist_names, st.session_state.playlist_links):\n                st.write(name + \" | \" + url)\n</code></pre>"},{"location":"recommender/#src.recommender.search_results_section","title":"<code>search_results_section()</code>","text":"<p>This methods creates the search results section in the Streamlit app.</p> <p>This can be broken down into two sub-sections. 1. Spotify Recommendations 2. The similarity visualization</p> <p>Note, if no playlist or name is given, this section is replaced by a <code>Please perform search first</code> message</p> Source code in <code>src/recommender.py</code> <pre><code>def search_results_section():\n    \"\"\"This methods creates the search results section in the Streamlit app.\n\n    This can be broken down into two sub-sections.\n    1. Spotify Recommendations\n    2. The similarity visualization\n\n    Note, if no playlist or name is given, this section is replaced by a `Please perform search first` message\n    \"\"\"\n    st.header('Search Results')\n    if st.session_state.similarity is None:\n        st.write('Please perform a search first')\n    else:\n        # Recommended Results\n        display_spotify_recommendations()\n\n        # Similarity visualization\n        fig_1 = similarity_visualization()\n        st.pyplot(fig_1)\n</code></pre>"},{"location":"recommender/#src.recommender.similarity_visualization","title":"<code>similarity_visualization()</code>","text":"<p>This method creates the similarity visualization of the dataset tracks to the created dataset feature vector.</p> <p>Returns:</p> Type Description <code>Pyplot Figure</code> <p>Figure for visualization by Streamlit</p> Source code in <code>src/recommender.py</code> <pre><code>def similarity_visualization():\n    \"\"\"This method creates the similarity visualization of the dataset tracks to the created dataset feature vector.\n\n    Returns:\n        (Pyplot Figure): Figure for visualization by Streamlit\n    \"\"\"\n    df = pd.DataFrame(st.session_state.similarity.access_similarity_scores(),\n                           columns=['sim_score'])  # Access similarity scores of the process\n\n    st.markdown(\"#### Playlist Similarity to Track Dataset\")\n    sns.set_style('whitegrid')\n    fig, axes = plt.subplots(1, 1, figsize=(12, 4))\n\n    g = sns.histplot(data=df,\n                     x='sim_score',\n                     kde=True,\n                     color='red',\n                     ax=axes)\n\n    axes.set_yscale('log')\n    axes.set_xscale('log')\n    axes.set_xlabel('Similarity Values (Log)')\n    axes.set_ylabel('Frequency')\n    return fig\n</code></pre>"},{"location":"similarity/","title":"Similarity","text":""},{"location":"similarity/#cosine-similarity","title":"Cosine Similarity","text":"<p>This file makes use of the track features generated by the Cosine Pipeline in order  to calculate the similarity between a playlist feature vector the a dataset of tracks. This enables a selection of the most similar tracks.</p> <p>Cosine Similarity inherits from the Similarity Interface.</p>"},{"location":"similarity/#cosine-similarity-documentation","title":"Cosine Similarity Documentation","text":""},{"location":"similarity/#src.similarity.TracksCosineSimilarity","title":"<code>TracksCosineSimilarity</code>","text":"<p>             Bases: <code>Similarity</code></p> <p>The class implements a Cosine similarity between a playlist vector and the tracks dataset to determine similar tracks to the playlist.</p> <p>This class inherits the Similarity interface.</p> <p>Attributes:</p> Name Type Description <code>additional_weighting</code> <code>int</code> <p>The weighting factor applied to weighted columns.</p> <code>playlist</code> <code>DataFrame</code> <p>The tracks dataset dataframe (before pipeline transformation)</p> <code>tracks</code> <code>DataFrame</code> <p>The playlist tracks dataframe (before pipeline transformation)</p> <code>playlist_features</code> <code>DataFrame</code> <p>The tracks dataset features (after transformation pipeline)</p> <code>track_features</code> <code>DataFrame</code> <p>The playlist tracks features (after transformation pipeline)</p> <code>similarity</code> <code>Series</code> <p>The ordered ranking of track similarity to the playlist vector (The index is uris)</p> Source code in <code>src/similarity.py</code> <pre><code>class TracksCosineSimilarity(Similarity):\n    \"\"\"The class implements a Cosine similarity between a playlist vector and the tracks dataset to determine\n        similar tracks to the playlist.\n\n        This class inherits the Similarity interface.\n\n        Attributes:\n            additional_weighting (int): The weighting factor applied to weighted columns.\n            playlist (DataFrame): The tracks dataset dataframe (before pipeline transformation)\n            tracks (DataFrame): The playlist tracks dataframe (before pipeline transformation)\n            playlist_features (DataFrame): The tracks dataset features (after transformation pipeline)\n            track_features (DataFrame): The playlist tracks features (after transformation pipeline)\n            similarity (Series): The ordered ranking of track similarity to the playlist vector (The index is uris)\n        \"\"\"\n    def __init__(self, playlist: pd.DataFrame, tracks: pd.DataFrame, weighted_features: list):\n        \"\"\"The initialization of the Cosine Similarity class\n\n        Args:\n            playlist (DataFrame): The tracks dataset dataframe (before pipeline transformation)\n            tracks (DataFrame): The playlist tracks dataframe (before pipeline transformation)\n            weighted_features (list): A list of features to be weighted in order to prioritize feature importance in similarity calculation.\n\n        \"\"\"\n        self.additional_weighting = 2  # Feature weighting value\n        features = CosinePipeline.data_pipeline(tracks)  # Pass data through pipeline to extract features\n\n        self.playlist = playlist\n        self.tracks = tracks\n\n        self.playlist_features, self.track_features = self.separate_playlist_from_tracks(features)\n        self.weight_features(weighted_features)\n        self.similarity = None\n\n    def calculate_similarity(self):\n        \"\"\"Method calculates the similarity between a playlist vector and the tracks feature matrix using cosine similarity\n\n        This calculation populates the `self.similarity` field.\n\n        The playlist feature dataframe is mean of each feature, creating a playlist vector.\n        \"\"\"\n        playlist_vector = self.vectorize_playlist()\n        track_matrix = self.track_features.to_numpy()\n\n        similarity_score = similarity_measures.cosine_similarity(track_matrix, playlist_vector)\n        uris = self.track_features.index.tolist()\n\n        self.similarity = pd.Series(similarity_score.T.tolist()[0], index=uris, name='sim_score')\n\n    def access_similarity_scores(self):\n        \"\"\"Getter method to access the `similarity` class field.\n\n        Returns:\n            (Series): The track similarity to the playlist vector. Similarity is a Series using uris as the index.\n        \"\"\"\n        return self.similarity\n\n    def get_top_n(self, n: int):\n        \"\"\"This method should return the top-n most similar tracks as a Dataframe with essential features included.\n\n        Note, due to the cosine similarity. A similarity value of 1 indicates a high similarity, while a value near 0 indicates a low similarity.\n\n        Args:\n            n (int): The top-n most similar tracks to the playlist vector\n\n        Returns:\n            (DataFrame): A dataframe containing the top-n tracks.\n        \"\"\"\n        sim_df = pd.DataFrame(self.similarity, columns=['sim_score'])\n        sorted_sim = sim_df.sort_values(by='sim_score', ascending=False)\n        sorted_top = sorted_sim.head(n)\n        return sorted_top.merge(self.tracks, left_index=True, right_on='uris')\n\n    def separate_playlist_from_tracks(self, features: pd.DataFrame):\n        \"\"\"Method separates the feature dataframe (from pipeline) into tracks and playlist feature dataframes\n\n        Args:\n            features (DataFrame): The track dataset features dataframe (This contains the playlist tracks too)\n\n        Returns:\n            playlist_features (Dataframe): The playlist track features dataframe\n            tracks_features (DataFrame): The track dataset features dataframe\n        \"\"\"\n        playlist_uris = self.playlist['uris'].tolist()\n        return features[features.index.isin(playlist_uris)], features[~features.index.isin(playlist_uris)]\n\n    def vectorize_playlist(self):\n        \"\"\"Method vectorizes the playlist track features by determining the mean value of each track feature\n\n        Returns:\n            (Numpy vector): The playlist feature vector\n        \"\"\"\n        playlist_vector = self.playlist_features.mean(axis=0)\n        return playlist_vector.to_numpy().reshape(1, -1)\n\n    def weight_features(self, weighted_columns: list):\n        \"\"\"Method weights the track dataset features (all features are normalized [0, 1]) by a scaler value\n        to increase the effect of that feature in the similarity calculation.\n\n        Wighting in cosine similarity increases the impact of the feature in the similarity calculation\n\n        Note, this method directly alters the `self.track_features` field.\n\n        Args:\n            weighted_columns: The columns to be weighted by the additional weighting factor.\n\n        \"\"\"\n        all_features = pd.Series(self.track_features.columns)\n        feature_filter = all_features.isin(weighted_columns).tolist()   # Boolean filter based on if feature is in weighted columns\n        binary_filter = [int(feature) for feature in feature_filter]  # Modify boolean filter into a binary filter\n        filler = [1] * len(binary_filter)  # Create a filler of 1's to not effect features that have no weighting\n        weights = [(self.additional_weighting * weight) + fill for weight, fill in zip(binary_filter, filler)]  # Calculate feature scaler weights\n\n        self.track_features = self.track_features.mul(weights, axis=1)  # Weight features\n</code></pre>"},{"location":"similarity/#src.similarity.TracksCosineSimilarity.__init__","title":"<code>__init__(playlist, tracks, weighted_features)</code>","text":"<p>The initialization of the Cosine Similarity class</p> <p>Parameters:</p> Name Type Description Default <code>playlist</code> <code>DataFrame</code> <p>The tracks dataset dataframe (before pipeline transformation)</p> required <code>tracks</code> <code>DataFrame</code> <p>The playlist tracks dataframe (before pipeline transformation)</p> required <code>weighted_features</code> <code>list</code> <p>A list of features to be weighted in order to prioritize feature importance in similarity calculation.</p> required Source code in <code>src/similarity.py</code> <pre><code>def __init__(self, playlist: pd.DataFrame, tracks: pd.DataFrame, weighted_features: list):\n    \"\"\"The initialization of the Cosine Similarity class\n\n    Args:\n        playlist (DataFrame): The tracks dataset dataframe (before pipeline transformation)\n        tracks (DataFrame): The playlist tracks dataframe (before pipeline transformation)\n        weighted_features (list): A list of features to be weighted in order to prioritize feature importance in similarity calculation.\n\n    \"\"\"\n    self.additional_weighting = 2  # Feature weighting value\n    features = CosinePipeline.data_pipeline(tracks)  # Pass data through pipeline to extract features\n\n    self.playlist = playlist\n    self.tracks = tracks\n\n    self.playlist_features, self.track_features = self.separate_playlist_from_tracks(features)\n    self.weight_features(weighted_features)\n    self.similarity = None\n</code></pre>"},{"location":"similarity/#src.similarity.TracksCosineSimilarity.access_similarity_scores","title":"<code>access_similarity_scores()</code>","text":"<p>Getter method to access the <code>similarity</code> class field.</p> <p>Returns:</p> Type Description <code>Series</code> <p>The track similarity to the playlist vector. Similarity is a Series using uris as the index.</p> Source code in <code>src/similarity.py</code> <pre><code>def access_similarity_scores(self):\n    \"\"\"Getter method to access the `similarity` class field.\n\n    Returns:\n        (Series): The track similarity to the playlist vector. Similarity is a Series using uris as the index.\n    \"\"\"\n    return self.similarity\n</code></pre>"},{"location":"similarity/#src.similarity.TracksCosineSimilarity.calculate_similarity","title":"<code>calculate_similarity()</code>","text":"<p>Method calculates the similarity between a playlist vector and the tracks feature matrix using cosine similarity</p> <p>This calculation populates the <code>self.similarity</code> field.</p> <p>The playlist feature dataframe is mean of each feature, creating a playlist vector.</p> Source code in <code>src/similarity.py</code> <pre><code>def calculate_similarity(self):\n    \"\"\"Method calculates the similarity between a playlist vector and the tracks feature matrix using cosine similarity\n\n    This calculation populates the `self.similarity` field.\n\n    The playlist feature dataframe is mean of each feature, creating a playlist vector.\n    \"\"\"\n    playlist_vector = self.vectorize_playlist()\n    track_matrix = self.track_features.to_numpy()\n\n    similarity_score = similarity_measures.cosine_similarity(track_matrix, playlist_vector)\n    uris = self.track_features.index.tolist()\n\n    self.similarity = pd.Series(similarity_score.T.tolist()[0], index=uris, name='sim_score')\n</code></pre>"},{"location":"similarity/#src.similarity.TracksCosineSimilarity.get_top_n","title":"<code>get_top_n(n)</code>","text":"<p>This method should return the top-n most similar tracks as a Dataframe with essential features included.</p> <p>Note, due to the cosine similarity. A similarity value of 1 indicates a high similarity, while a value near 0 indicates a low similarity.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>The top-n most similar tracks to the playlist vector</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A dataframe containing the top-n tracks.</p> Source code in <code>src/similarity.py</code> <pre><code>def get_top_n(self, n: int):\n    \"\"\"This method should return the top-n most similar tracks as a Dataframe with essential features included.\n\n    Note, due to the cosine similarity. A similarity value of 1 indicates a high similarity, while a value near 0 indicates a low similarity.\n\n    Args:\n        n (int): The top-n most similar tracks to the playlist vector\n\n    Returns:\n        (DataFrame): A dataframe containing the top-n tracks.\n    \"\"\"\n    sim_df = pd.DataFrame(self.similarity, columns=['sim_score'])\n    sorted_sim = sim_df.sort_values(by='sim_score', ascending=False)\n    sorted_top = sorted_sim.head(n)\n    return sorted_top.merge(self.tracks, left_index=True, right_on='uris')\n</code></pre>"},{"location":"similarity/#src.similarity.TracksCosineSimilarity.separate_playlist_from_tracks","title":"<code>separate_playlist_from_tracks(features)</code>","text":"<p>Method separates the feature dataframe (from pipeline) into tracks and playlist feature dataframes</p> <p>Parameters:</p> Name Type Description Default <code>features</code> <code>DataFrame</code> <p>The track dataset features dataframe (This contains the playlist tracks too)</p> required <p>Returns:</p> Name Type Description <code>playlist_features</code> <code>Dataframe</code> <p>The playlist track features dataframe</p> <code>tracks_features</code> <code>DataFrame</code> <p>The track dataset features dataframe</p> Source code in <code>src/similarity.py</code> <pre><code>def separate_playlist_from_tracks(self, features: pd.DataFrame):\n    \"\"\"Method separates the feature dataframe (from pipeline) into tracks and playlist feature dataframes\n\n    Args:\n        features (DataFrame): The track dataset features dataframe (This contains the playlist tracks too)\n\n    Returns:\n        playlist_features (Dataframe): The playlist track features dataframe\n        tracks_features (DataFrame): The track dataset features dataframe\n    \"\"\"\n    playlist_uris = self.playlist['uris'].tolist()\n    return features[features.index.isin(playlist_uris)], features[~features.index.isin(playlist_uris)]\n</code></pre>"},{"location":"similarity/#src.similarity.TracksCosineSimilarity.vectorize_playlist","title":"<code>vectorize_playlist()</code>","text":"<p>Method vectorizes the playlist track features by determining the mean value of each track feature</p> <p>Returns:</p> Type Description <code>Numpy vector</code> <p>The playlist feature vector</p> Source code in <code>src/similarity.py</code> <pre><code>def vectorize_playlist(self):\n    \"\"\"Method vectorizes the playlist track features by determining the mean value of each track feature\n\n    Returns:\n        (Numpy vector): The playlist feature vector\n    \"\"\"\n    playlist_vector = self.playlist_features.mean(axis=0)\n    return playlist_vector.to_numpy().reshape(1, -1)\n</code></pre>"},{"location":"similarity/#src.similarity.TracksCosineSimilarity.weight_features","title":"<code>weight_features(weighted_columns)</code>","text":"<p>Method weights the track dataset features (all features are normalized [0, 1]) by a scaler value to increase the effect of that feature in the similarity calculation.</p> <p>Wighting in cosine similarity increases the impact of the feature in the similarity calculation</p> <p>Note, this method directly alters the <code>self.track_features</code> field.</p> <p>Parameters:</p> Name Type Description Default <code>weighted_columns</code> <code>list</code> <p>The columns to be weighted by the additional weighting factor.</p> required Source code in <code>src/similarity.py</code> <pre><code>def weight_features(self, weighted_columns: list):\n    \"\"\"Method weights the track dataset features (all features are normalized [0, 1]) by a scaler value\n    to increase the effect of that feature in the similarity calculation.\n\n    Wighting in cosine similarity increases the impact of the feature in the similarity calculation\n\n    Note, this method directly alters the `self.track_features` field.\n\n    Args:\n        weighted_columns: The columns to be weighted by the additional weighting factor.\n\n    \"\"\"\n    all_features = pd.Series(self.track_features.columns)\n    feature_filter = all_features.isin(weighted_columns).tolist()   # Boolean filter based on if feature is in weighted columns\n    binary_filter = [int(feature) for feature in feature_filter]  # Modify boolean filter into a binary filter\n    filler = [1] * len(binary_filter)  # Create a filler of 1's to not effect features that have no weighting\n    weights = [(self.additional_weighting * weight) + fill for weight, fill in zip(binary_filter, filler)]  # Calculate feature scaler weights\n\n    self.track_features = self.track_features.mul(weights, axis=1)  # Weight features\n</code></pre>"},{"location":"similarity_interface/","title":"Similarity interface","text":""},{"location":"similarity_interface/#similarity-interface","title":"Similarity (Interface)","text":"<p>This file provides the framework for various methods of track similarity calculations. Each similarity concept/ idea must inherent from the similarity interface. </p>"},{"location":"similarity_interface/#similarity-interface-documentation","title":"Similarity Interface Documentation","text":""},{"location":"similarity_interface/#src.similarity_interface.Similarity","title":"<code>Similarity</code>","text":"<p>             Bases: <code>ABC</code></p> <p>This class serves as an outline of the Similarity module for the MIAS application</p> <p>There are multiple ways of achieving a recommendation system/ similarity of tracks. This modular build allows others to add their own classes to try out various methods in a simple way</p> <p>Note: The playlist has already been added to the tracks dataset, such that features are calculated over the entire database. The playlist tracks are then removed from the tracks dataaset in the similarity calculation.</p> <p>Additionally: It is recommended that your class take in both <code>playlist</code> and <code>tracks</code> as done in the Cosine Similarity Class</p> Source code in <code>src/similarity_interface.py</code> <pre><code>class Similarity(ABC):\n    \"\"\"This class serves as an outline of the Similarity module for the MIAS application\n\n    There are multiple ways of achieving a recommendation system/ similarity of tracks.\n    This modular build allows others to add their own classes to try out various methods in\n    a simple way\n\n    Note: The playlist has already been added to the tracks dataset, such that features are\n    calculated over the entire database. The playlist tracks are then removed from the tracks dataaset\n    in the similarity calculation.\n\n    Additionally: It is recommended that your class take in both `playlist` and `tracks` as done\n    in the Cosine Similarity Class\n    \"\"\"\n    @abstractmethod\n    def calculate_similarity(self):\n        \"\"\"Method calculates the similarity of each track in the dataset to the given playlist feature vector\n\n        Note, this method should populate a field called `self.similarity`, such that it represents a similarity score of\n        each track to playlist vector. Please see `similarity.py` for an example implementation.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def access_similarity_scores(self):\n        \"\"\"This method is a getter method providing access to the similarity metrics calculated in `calculate_similarity()`\"\"\"\n        pass\n\n    @abstractmethod\n    def get_top_n(self, n: int):\n        \"\"\"This method should return the top-n most similar tracks as a Dataframe with essential features included.\n        Note, see `similarity.py` for an example implementation\n        Note, depending on your similarity calculation, be careful of the type of ordering you implement.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def weight_features(self, weighted_columns: list):\n        \"\"\"This method allows for specific features to be weighted or have a more significant impact on the measures of similarity.\n        Note, weighting may vary depending on the method of similarity calculation. Please see `similarity.py` for an example\n        implementation\n        \"\"\"\n        pass\n</code></pre>"},{"location":"similarity_interface/#src.similarity_interface.Similarity.access_similarity_scores","title":"<code>access_similarity_scores()</code>  <code>abstractmethod</code>","text":"<p>This method is a getter method providing access to the similarity metrics calculated in <code>calculate_similarity()</code></p> Source code in <code>src/similarity_interface.py</code> <pre><code>@abstractmethod\ndef access_similarity_scores(self):\n    \"\"\"This method is a getter method providing access to the similarity metrics calculated in `calculate_similarity()`\"\"\"\n    pass\n</code></pre>"},{"location":"similarity_interface/#src.similarity_interface.Similarity.calculate_similarity","title":"<code>calculate_similarity()</code>  <code>abstractmethod</code>","text":"<p>Method calculates the similarity of each track in the dataset to the given playlist feature vector</p> <p>Note, this method should populate a field called <code>self.similarity</code>, such that it represents a similarity score of each track to playlist vector. Please see <code>similarity.py</code> for an example implementation.</p> Source code in <code>src/similarity_interface.py</code> <pre><code>@abstractmethod\ndef calculate_similarity(self):\n    \"\"\"Method calculates the similarity of each track in the dataset to the given playlist feature vector\n\n    Note, this method should populate a field called `self.similarity`, such that it represents a similarity score of\n    each track to playlist vector. Please see `similarity.py` for an example implementation.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"similarity_interface/#src.similarity_interface.Similarity.get_top_n","title":"<code>get_top_n(n)</code>  <code>abstractmethod</code>","text":"<p>This method should return the top-n most similar tracks as a Dataframe with essential features included. Note, see <code>similarity.py</code> for an example implementation Note, depending on your similarity calculation, be careful of the type of ordering you implement.</p> Source code in <code>src/similarity_interface.py</code> <pre><code>@abstractmethod\ndef get_top_n(self, n: int):\n    \"\"\"This method should return the top-n most similar tracks as a Dataframe with essential features included.\n    Note, see `similarity.py` for an example implementation\n    Note, depending on your similarity calculation, be careful of the type of ordering you implement.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"similarity_interface/#src.similarity_interface.Similarity.weight_features","title":"<code>weight_features(weighted_columns)</code>  <code>abstractmethod</code>","text":"<p>This method allows for specific features to be weighted or have a more significant impact on the measures of similarity. Note, weighting may vary depending on the method of similarity calculation. Please see <code>similarity.py</code> for an example implementation</p> Source code in <code>src/similarity_interface.py</code> <pre><code>@abstractmethod\ndef weight_features(self, weighted_columns: list):\n    \"\"\"This method allows for specific features to be weighted or have a more significant impact on the measures of similarity.\n    Note, weighting may vary depending on the method of similarity calculation. Please see `similarity.py` for an example\n    implementation\n    \"\"\"\n    pass\n</code></pre>"}]}